{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXWMYszuNFYDqRXiifV8M4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamyraOliveiraC/Pequeno-Projeto-Web-Squaping/blob/main/PequenoProjeto_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l2gNuJdLqgh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b23f9bf-8326-42a5-841c-96f22608972a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "‚úÖ Dados coletados e salvos em 'dados_coletados.txt'\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Cria um arquivo para guardar as informa√ß√µes\n",
        "with open(\"dados_coletados.txt\", \"w\", encoding=\"utf-8\") as arquivo:\n",
        "\n",
        "    # 1. üì∞ NOT√çCIAS do G1\n",
        "    url_g1 = \"https://g1.globo.com/\"\n",
        "    resposta_g1 = requests.get(url_g1)\n",
        "    pagina_g1 = BeautifulSoup(resposta_g1.text, \"html.parser\")\n",
        "    noticias = pagina_g1.find_all(\"a\", class_=\"feed-post-link\")\n",
        "\n",
        "    arquivo.write(\"üì∞ NOT√çCIAS DO G1:\\n\")\n",
        "    for noticia in noticias[:5]:\n",
        "        texto = noticia.get_text().strip()\n",
        "        arquivo.write(\"- \" + texto + \"\\n\")\n",
        "    arquivo.write(\"\\n\")\n",
        "\n",
        "    # 2. ‚òÄÔ∏è CLIMA de S√£o Paulo no ClimaTempo\n",
        "    url_clima = \"https://www.climatempo.com.br/previsao-do-tempo/cidade/558/saopaulo-sp\"\n",
        "    resposta_clima = requests.get(url_clima)\n",
        "    pagina_clima = BeautifulSoup(resposta_clima.text, \"html.parser\")\n",
        "    clima_element = pagina_clima.find(\"h1\", class_=\"titulo-mapa\")\n",
        "    temperatura_element = pagina_clima.find(\"span\", class_=\"temperature _margin-l-5 -bold\")\n",
        "\n",
        "    arquivo.write(\"‚òÄÔ∏è CLIMA EM S√ÉO PAULO:\\n\")\n",
        "    if clima_element and temperatura_element:\n",
        "        clima = clima_element.get_text().strip()\n",
        "        temperatura = temperatura_element.get_text().strip()\n",
        "        arquivo.write(f\"{clima} - Temperatura: {temperatura}\\n\\n\")\n",
        "    else:\n",
        "        arquivo.write(\"N√£o foi poss√≠vel obter as informa√ß√µes do clima.\\n\\n\")\n",
        "\n",
        "\n",
        "    # 3. üíª PRODUTOS da Kabum (busca por 'monitor')\n",
        "    url_kabum = \"https://www.kabum.com.br/busca/monitor\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Kabum exige cabe√ßalho\n",
        "    resposta_kabum = requests.get(url_kabum, headers=headers)\n",
        "    pagina_kabum = BeautifulSoup(resposta_kabum.text, \"html.parser\")\n",
        "    produtos = pagina_kabum.find_all(\"div\", class_=\"productCard\")\n",
        "\n",
        "    arquivo.write(\"üíª PRODUTOS DA KABUM (monitores):\\n\")\n",
        "    for produto in produtos[:5]:\n",
        "        nome_element = produto.find(\"span\", class_=\"nameCard\")\n",
        "        preco_element = produto.find(\"span\", class_=\"priceCard\")\n",
        "        if nome_element and preco_element:\n",
        "            nome = nome_element.get_text().strip()\n",
        "            preco = preco_element.get_text().strip()\n",
        "            arquivo.write(f\"- {nome} | {preco}\\n\")\n",
        "        else:\n",
        "             arquivo.write(f\"- N√£o foi poss√≠vel obter informa√ß√µes do produto.\\n\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Dados coletados e salvos em 'dados_coletados.txt'\")"
      ]
    }
  ]
}